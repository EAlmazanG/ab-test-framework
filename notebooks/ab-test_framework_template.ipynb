{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import importlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pingouin as pg\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "from scipy.stats import ttest_ind, mannwhitneyu, f_oneway, kruskal, chi2_contingency, fisher_exact\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_title(title, line_length = 60, symbol = '-'):\n",
    "    separator = symbol * ((line_length - len(title) - 2) // 2)\n",
    "    print(f\"{separator} {title} {separator}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT DESIGN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOURCES INGESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOURCES INGESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "file_name = 'ab_test_example_3.csv'\n",
    "df_raw = pd.read_csv('../data/' + file_name)\n",
    "display(df_raw.head(5))\n",
    "\n",
    "# Make a copy\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Check dtypes\n",
    "print_title('INITIAL DATA TYPES')\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_columns(df, datetime_columns=[], int64_columns=[], float64_columns=[], str_columns=[]):\n",
    "    for col in datetime_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "    for col in int64_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype('Int64')  \n",
    "\n",
    "    for col in float64_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype('float64')\n",
    "\n",
    "    for col in str_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype('str')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_columns = ['WHEN_ENTERED_INTO_EXPERIMENT']\n",
    "int64_columns = ['U_ID', 'PROMOTER_U_ID', 'IS_TREATMENT', 'COUNT_TRANSFERS', 'DAYS_TRANSACTING', 'DISTINCT_RECEIVERS']\n",
    "float64_columns = ['TOTAL_TRANSFER_AMOUNT']\n",
    "str_columns = ['VARIANT', 'USER_SEGMENT', 'PROMO_TAG']\n",
    "\n",
    "# Basic data conversion\n",
    "df = format_columns(df, datetime_columns, int64_columns, float64_columns, str_columns)\n",
    "\n",
    "# Check dtypes\n",
    "print_title('CONVERTED DATA TYPES')\n",
    "print(df.dtypes)\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_column = 'VARIANT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick checks on data\n",
    "print_title('DF INFO')\n",
    "display(df.info())\n",
    "\n",
    "print_title('DF DESCRIBE')\n",
    "display(df.describe())\n",
    "\n",
    "# Check distribution of variants\n",
    "print_title('VARIANT DISTRIBUTION')\n",
    "display(df[variant_column].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DUPLICATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for and drop duplicates in the entire DataFrame\n",
    "duplicated_rows = df.duplicated().sum()\n",
    "print('# of duplicated rows: ', duplicated_rows)\n",
    "\n",
    "if duplicated_rows > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print('Duplicates in the DataFrame removed.')\n",
    "else:\n",
    "    print('No duplicates in the DataFrame found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key_column = 'U_ID'\n",
    "timestamp_column = ''\n",
    "\n",
    "# Check for duplicates in the unique columns\n",
    "duplicated_rows = df[df[primary_key_column].duplicated(keep=False)]\n",
    "print(f'# of duplicated on {primary_key_column} column: {duplicated_rows[primary_key_column].nunique()}')\n",
    "\n",
    "if not duplicated_rows.empty:\n",
    "    print(f'Duplicated {primary_key_column} and their rows:')\n",
    "    display(duplicated_rows.sort_values(by = 'U_ID'))\n",
    "\n",
    "    # Keep only the first following timestamp column order\n",
    "    if timestamp_column == '':\n",
    "        df = df.drop_duplicates(subset=primary_key_column, keep='last')\n",
    "        print('Kept the most recent row for each duplicated U_ID.')\n",
    "    else:\n",
    "        df = df.sort_values(timestamp_column).drop_duplicates(subset=primary_key_column, keep='last')\n",
    "        print('Kept the most recent row for each duplicated U_ID.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NULLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print_title('NUMBER OF NULL VALUES')\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null columns\n",
    "df['TOTAL_TRANSFER_AMOUNT'] = df['TOTAL_TRANSFER_AMOUNT'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INCONSISTENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks errors in variant labeling\n",
    "checks = df[(df['IS_TREATMENT'] == 1) & (df['VARIANT'] != 'PRICE_PROMO')]\n",
    "display(checks)\n",
    "checks = df[(df['IS_TREATMENT'] == 0) & (df['VARIANT'] != 'CONTROL')]\n",
    "display(checks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METRICS DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the metrics, use metric_cnt_ or metric_cvr_\n",
    "df['metric_cnt_total_transfer_amount'] = df['TOTAL_TRANSFER_AMOUNT']\n",
    "df['metric_cvr_transaction'] = df['TOTAL_TRANSFER_AMOUNT'].apply(lambda x: 1 if x > 0 else 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, metric_column, factor=1.5, threshold=0.1):\n",
    "    q1 = df[metric_column].quantile(0.2)\n",
    "    q3 = df[metric_column].quantile(0.8)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - factor * iqr\n",
    "    upper_bound = q3 + factor * iqr\n",
    "\n",
    "    print(\"Low outlier limit:\", lower_bound)\n",
    "    print(\"Upper outlier limit:\", upper_bound)\n",
    "\n",
    "    initial_count = df.shape[0]\n",
    "    df_filtered = df.loc[(df[metric_column] >= lower_bound) & (df[metric_column] <= upper_bound)]\n",
    "    final_count = df_filtered.shape[0]\n",
    "    removed_percentage = (initial_count - final_count) / initial_count\n",
    "\n",
    "    print(f\"Filtered {initial_count - final_count} rows ({removed_percentage:.2%}) from {initial_count} to {final_count}\")\n",
    "\n",
    "    is_strong_outlier_effect = removed_percentage > threshold\n",
    "    print(f\"is_strong_outlier_effect: {is_strong_outlier_effect}\")\n",
    "\n",
    "    return df_filtered, is_strong_outlier_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select metrics and columns involved in the test\n",
    "primary_key_column = 'U_ID'\n",
    "metric_column = 'metric_cnt_total_transfer_amount'\n",
    "variant_column = 'VARIANT'\n",
    "\n",
    "columns_selection_df = df[[primary_key_column, variant_column, metric_column]]\n",
    "metric_type = (\n",
    "    'continuous' if metric_column.startswith('metric_cnt_') else\n",
    "    'proportion' if metric_column.startswith('metric_cvr_') else\n",
    "    None\n",
    ")\n",
    "outliers_filtered_df, is_strong_outlier_effect = remove_outliers(columns_selection_df, metric_column, 1)\n",
    "\n",
    "# Filter outliers:\n",
    "filter_outliers = False\n",
    "\n",
    "if filter_outliers:\n",
    "    selected_df = outliers_filtered_df.copy()\n",
    "else:\n",
    "    selected_df = columns_selection_df.copy()\n",
    "\n",
    "display(selected_df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUMBER OF VARIANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple variants\n",
    "np.random.seed(42)\n",
    "num_rows = len(df)\n",
    "num_to_change = num_rows // 3\n",
    "indices_to_change = np.random.choice(df.index, size=num_to_change, replace=False)\n",
    "selected_df.loc[indices_to_change, \"VARIANT\"] = \"TEST_VARIANT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_variants = selected_df[variant_column].nunique()\n",
    "\n",
    "print(f\"Number of Variants: {num_variants}\")\n",
    "print(f\"Variants: {selected_df[variant_column].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAMPLE SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the sample size is large enough\n",
    "# check if the variant sizes ar equal or not and the proportion\n",
    "\n",
    "sample_sizes = selected_df[variant_column].value_counts()\n",
    "print(\"Sample sizes per variant:\")\n",
    "print(sample_sizes)\n",
    "\n",
    "variant_proportion = sample_sizes / sample_sizes.sum()\n",
    "print(\"\\nProportion per variant:\")\n",
    "print(variant_proportion)\n",
    "\n",
    "variant_ratio = sample_sizes.max() / sample_sizes.min()\n",
    "print(f\"\\nVariant Ratio (N = max/min): {variant_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAMPLE DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "\n",
    "def remove_axes_frame(ax):\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "def calculate_distribution(selected_df, variant_column, metric_column):\n",
    "    results = {}\n",
    "    variants = selected_df[variant_column].unique()\n",
    "    for variant in variants:\n",
    "        metric_data = selected_df.loc[selected_df[variant_column] == variant, metric_column]\n",
    "        if len(metric_data) < 5000:\n",
    "            stat, p_value = stats.shapiro(metric_data)\n",
    "            test_name = 'shapiro'\n",
    "        else:\n",
    "            stat, p_value = stats.normaltest(metric_data)\n",
    "            test_name = 'normaltest'\n",
    "        results[variant] = {'test': test_name, 'stat': stat, 'p_value': p_value}\n",
    "        print(f\"variant {variant}: {test_name} statistic = {stat:.4f}, p-value = {p_value:.4f}\")\n",
    "    return results\n",
    "\n",
    "def plot_qq(selected_df, variant_column, metric_column):\n",
    "    variants = selected_df[variant_column].unique()\n",
    "    n = len(variants)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(6 * n, 4))\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for ax, variant in zip(axes, variants):\n",
    "        metric_data = selected_df.loc[selected_df[variant_column] == variant, metric_column]\n",
    "        stats.probplot(metric_data, dist=\"norm\", plot=ax)\n",
    "        ax.set_title(f'qq plot - {variant}', fontsize=12)\n",
    "        remove_axes_frame(ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_histogram_kde(selected_df, variant_column, metric_column):\n",
    "    variants = selected_df[variant_column].unique()\n",
    "    n = len(variants)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(6 * n, 4))\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for ax, variant in zip(axes, variants):\n",
    "        metric_data = selected_df.loc[selected_df[variant_column] == variant, metric_column]\n",
    "        sns.histplot(\n",
    "            metric_data,\n",
    "            kde=True,\n",
    "            ax=ax,\n",
    "            color='skyblue',\n",
    "            stat='density',\n",
    "            edgecolor=None,\n",
    "            alpha=0.7\n",
    "        )\n",
    "        ax.set_title(f'histogram - {variant}', fontsize=12)\n",
    "        ax.set_xlabel(metric_column)\n",
    "        ax.set_ylabel('density')\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        remove_axes_frame(ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_violin(selected_df, variant_column, metric_column):\n",
    "    variants = selected_df[variant_column].unique()\n",
    "    n = len(variants)\n",
    "    fig, ax = plt.subplots(figsize=(6 * n, 4))\n",
    "    sns.violinplot(\n",
    "        x=variant_column,\n",
    "        y=metric_column,\n",
    "        data=selected_df,\n",
    "        hue=variant_column,\n",
    "        palette='pastel',\n",
    "        inner='quartile',\n",
    "        dodge=False,\n",
    "        ax=ax\n",
    "    )\n",
    "    if ax.get_legend() is not None:\n",
    "        ax.get_legend().remove()\n",
    "    ax.set_title('violin plot - metric distribution', fontsize=12)\n",
    "    ax.set_xlabel('variant')\n",
    "    ax.set_ylabel(metric_column)\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    remove_axes_frame(ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_combined_kde(selected_df, variant_column, metric_column):\n",
    "    variants = selected_df[variant_column].unique()\n",
    "    n = len(variants)\n",
    "    fig, ax = plt.subplots(figsize=(6 * n, 4))\n",
    "    for variant in variants:\n",
    "        metric_data = selected_df.loc[selected_df[variant_column] == variant, metric_column]\n",
    "        sns.kdeplot(\n",
    "            metric_data,\n",
    "            fill=True,\n",
    "            label=variant,\n",
    "            alpha=0.6,\n",
    "            ax=ax\n",
    "        )\n",
    "    ax.set_title('combined kde - metric distribution', fontsize=12)\n",
    "    ax.set_xlabel(metric_column)\n",
    "    ax.set_ylabel('density')\n",
    "    ax.legend(title='variant')\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    remove_axes_frame(ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def set_normal_distribution_flag(distribution_results, alpha=0.05):\n",
    "    # Set flag for normal distribution based on p_value > alpha\n",
    "    for variant, result in distribution_results.items():\n",
    "        is_normal_distribution = result['p_value'] > alpha\n",
    "    return is_normal_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_title('NORMAL DISTRIBUTION VISUAL ANALYSIS', 180)\n",
    "plot_qq(selected_df, variant_column, metric_column)\n",
    "plot_histogram_kde(selected_df, variant_column, metric_column)\n",
    "plot_violin(selected_df, variant_column, metric_column)\n",
    "plot_combined_kde(selected_df, variant_column, metric_column)\n",
    "\n",
    "print_title('NORMAL DISTRIBUTION TEST RESULTS', 180)\n",
    "distribution_results = calculate_distribution(selected_df, variant_column, metric_column)\n",
    "\n",
    "is_normal_distribution = set_normal_distribution_flag(distribution_results, alpha=0.05)\n",
    "print(f'\\nUSE NORMAL DISTRIBUTION TESTS: {is_normal_distribution}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAMPLE VARIANCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance_analysis(selected_df, variant_column, metric_column, alpha=0.05):\n",
    "    # Levene's test for homogeneity of variances\n",
    "    variants = selected_df[variant_column].unique()\n",
    "    groups = [selected_df.loc[selected_df[variant_column] == variant, metric_column] for variant in variants]\n",
    "    stat, p_value = stats.levene(*groups, center='median')\n",
    "    is_equal_variance = p_value > alpha\n",
    "    print(f\"Levene test statistic = {stat:.4f}, p_value = {p_value:.4f}\")\n",
    "    print(f\"Equal variance assumption: {is_equal_variance}\")\n",
    "    return {'test': 'levene', 'stat': stat, 'p_value': p_value, 'is_equal_variance': is_equal_variance}\n",
    "\n",
    "def set_equal_variance_flag(variance_results, alpha=0.05):\n",
    "    # Set flag for equal variance based on p_value > alpha\n",
    "    is_equal_variance = variance_results['p_value'] > alpha\n",
    "    return is_equal_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_title('VARIANCE TEST RESULTS', 180)\n",
    "variance_results = calculate_variance_analysis(selected_df, variant_column, metric_column)\n",
    "\n",
    "is_equal_variance = set_equal_variance_flag(variance_results, alpha=0.05)\n",
    "print(f'\\nUSE EQUAL VARIANCE TESTS: {is_equal_variance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STATISTICAL TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST AND TECHNICHES SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_ab_test(metric_type, is_equal_variance, is_normal_distribution, num_variants, variant_ratio, sample_sizes, is_strong_outlier_effect, size_threshold = 30):\n",
    "    print(f'metric_type: {metric_type}')\n",
    "    print(f'is_normal_distribution: {is_normal_distribution}')\n",
    "    print(f'is_equal_variance: {is_equal_variance}')\n",
    "    print(f'num_variants: {num_variants}')\n",
    "    print(f'sample_sizes: {sample_sizes}')\n",
    "    print(f'variant_ratio: {np.round(variant_ratio, 2)}')\n",
    "    print(f'is_strong_outlier_effect: {is_strong_outlier_effect}')\n",
    "    \n",
    "    ab_test_config = {\n",
    "        # Bi variant tests\n",
    "        \"use_mann_whitney_u_test\": False, # continuous, not normal, works with unbalanced samples\n",
    "        \"use_welchs_t_test\": False, # continuous, normal, different variances, works with unbalanced samples (if low unbalance applied although same variants)\n",
    "        \"use_t_test\": False, # continuous, normal, equal variances, not recommended if unbalanced\n",
    "        \"use_fisher_exact_test\": False, # proportions, small sample size, works with unbalanced samples, # for proportions, only size matters due to central limit theorem\n",
    "        \"use_two_proportion_z_test\": False, # proportions, large sample size, works with unbalanced samples, used alone or post Pearson Chi-square with Bonferroni for multiple variants, # for proportions, only size matters due to central limit theorem\n",
    "        ## Multiple variants tests\n",
    "        \"use_anova_test\": False, # continuous, normal, not recommended if unbalanced, equal variance\n",
    "        \"use_welch_anova_test\": False, # continuous, normal, if unbalanced, different variance\n",
    "        \"use_kruskal_wallis_test\": False, # continuous, not normal, works with unbalanced samples\n",
    "        \"use_pearson_chi_square_test\": False, # proportions, works with unbalanced samples but needs correction\n",
    "        ## Multiple variants post-pairs tests\n",
    "        \"use_tukey_hsd_test\": False, # post anova, continuous, not recommended if unbalanced\n",
    "        \"use_games_howell_test\": False, # post anova, continuous, for unbalanced samples\n",
    "        \"use_dunn_test\": False, # post kruskal wallis, continuous, needs bonferroni, works with unbalanced samples\n",
    "        ## Multiple variants correction\n",
    "        \"use_bonferroni_correction\": False, # if more than 2 variants and tukey pr games howell is not used\n",
    "        ## Unbalance data: N = A/B, if N < 2.5 use normal testing, if N < 5x use balance resampling, if N > 5x use bootstraping\n",
    "        \"use_balance_resampling\": False, # for unbalanced sample sizes, equalizing groups, downsampling,\n",
    "        \"use_bootstraping\": False, # for small sample sizes or unbalanced groups, estimating confidence intervals, upsampling,\n",
    "        ## Additional techniques\n",
    "        \"use_bayesian_test\": False, # for probabilistic interpretation, alternative to p-values, small samples\n",
    "        \"use_permutation_test\": False # for distribution-free significance testing, alternative to t-tests or z-tests, extrange distributions\n",
    "    }\n",
    "    \n",
    "    # Unbalance techniques\n",
    "    if variant_ratio >= 2.5 and variant_ratio < 5:\n",
    "        ab_test_config[\"use_balance_resampling\"] = True\n",
    "    elif variant_ratio >= 5:\n",
    "        ab_test_config[\"use_bootstraping\"] = True\n",
    "\n",
    "    if metric_type == 'continuous':\n",
    "        # Bi variant tests\n",
    "        if num_variants == 2:\n",
    "            if not is_normal_distribution:\n",
    "                ab_test_config[\"use_mann_whitney_u_test\"] = True\n",
    "            else:\n",
    "                if is_equal_variance:\n",
    "                    if variant_ratio < 1.5:\n",
    "                        ab_test_config[\"use_t_test\"] = True \n",
    "                    elif 1.5 <= variant_ratio <= 2.5:\n",
    "                        ab_test_config[\"use_welchs_t_test\"] = True \n",
    "                    else:  \n",
    "                        ab_test_config[\"use_t_test\"] = True\n",
    "                else:\n",
    "                    ab_test_config[\"use_welchs_t_test\"] = True \n",
    "        # Multiple variant tests\n",
    "        elif num_variants > 2:\n",
    "            if is_normal_distribution:\n",
    "                if is_equal_variance:\n",
    "                    if variant_ratio < 1.5:\n",
    "                        ab_test_config[\"use_anova_test\"] = True\n",
    "                        ab_test_config[\"use_tukey_hsd_test\"] = True\n",
    "                    elif 1.5 <= variant_ratio <= 2.5:\n",
    "                        ab_test_config[\"use_welch_anova_test\"] = True\n",
    "                        ab_test_config[\"use_games_howell_test\"] = True\n",
    "                    else:  # variant_ratio > 2.5\n",
    "                        ab_test_config[\"use_anova_test\"] = True\n",
    "                        ab_test_config[\"use_tukey_hsd_test\"] = True \n",
    "                else:\n",
    "                    ab_test_config[\"use_welch_anova_test\"] = True\n",
    "                    ab_test_config[\"use_games_howell_test\"] = True  \n",
    "            else:\n",
    "                ab_test_config[\"use_kruskal_wallis_test\"] = True\n",
    "                ab_test_config[\"use_dunn_test\"] = True\n",
    "            \n",
    "            if not (ab_test_config[\"use_tukey_hsd_test\"] or \n",
    "                    ab_test_config[\"use_games_howell_test\"]) or ab_test_config[\"use_dunn_test\"]:\n",
    "                ab_test_config[\"use_bonferroni_correction\"] = True\n",
    "\n",
    "    # Proportions metric\n",
    "    elif metric_type == 'proportion':\n",
    "        # Bi variant tests\n",
    "        if num_variants == 2:\n",
    "            if sample_sizes.min() < size_threshold:\n",
    "                ab_test_config[\"use_fisher_exact_test\"] = True\n",
    "            else:\n",
    "                ab_test_config[\"use_two_proportion_z_test\"] = True\n",
    "        # Multiple variant tests\n",
    "        elif num_variants > 2:\n",
    "            ab_test_config[\"use_pearson_chi_square_test\"] = True\n",
    "            ab_test_config[\"use_two_proportion_z_test\"] = True\n",
    "            ab_test_config[\"use_bonferroni_correction\"] = True\n",
    "\n",
    "    # Additional techniques for small sample sizes (threshold < size_threshold)\n",
    "    if sample_sizes.min() < 1000 or not is_normal_distribution:\n",
    "        ab_test_config[\"use_bayesian_test\"] = True\n",
    "        \n",
    "    if sample_sizes.min() < 100 or (is_strong_outlier_effect and sample_sizes.min() < 2000):\n",
    "        ab_test_config[\"use_permutation_test\"] = True\n",
    "    return ab_test_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_title('TEST VARIABLES', 60)\n",
    "ab_test_config = configure_ab_test(metric_type, is_equal_variance, is_normal_distribution, num_variants, variant_ratio, sample_sizes, is_strong_outlier_effect)\n",
    "print('\\n')\n",
    "print_title('TEST SELECTION', 60)\n",
    "print({key: value for key, value in ab_test_config.items() if value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNBALANCE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_t_test(df, variant_column, metric_column):\n",
    "    variants = df[variant_column].unique()\n",
    "    groupA = df[df[variant_column] == variants[0]][metric_column]\n",
    "    groupB = df[df[variant_column] == variants[1]][metric_column]\n",
    "    stat, p_value = ttest_ind(groupA, groupB, equal_var=True)\n",
    "    return {\"stat\": stat, \"p_value\": p_value}\n",
    "\n",
    "def run_welchs_t_test(df, variant_column, metric_column):\n",
    "    variants = df[variant_column].unique()\n",
    "    groupA = df[df[variant_column] == variants[0]][metric_column]\n",
    "    groupB = df[df[variant_column] == variants[1]][metric_column]\n",
    "    stat, p_value = ttest_ind(groupA, groupB, equal_var=False)\n",
    "    return {\"stat\": stat, \"p_value\": p_value}\n",
    "\n",
    "def run_mann_whitney(df, variant_column, metric_column):\n",
    "    variants = df[variant_column].unique()\n",
    "    groupA = df[df[variant_column] == variants[0]][metric_column]\n",
    "    groupB = df[df[variant_column] == variants[1]][metric_column]\n",
    "    stat, p_value = mannwhitneyu(groupA, groupB, alternative='two-sided')\n",
    "    return {\"stat\": stat, \"p_value\": p_value}\n",
    "\n",
    "def run_two_proportion_z_test(df, variant_column, metric_column):\n",
    "    success_counts = df.groupby(variant_column)[metric_column].sum().values\n",
    "    total_counts = df.groupby(variant_column)[metric_column].count().values\n",
    "    stat, p_value = proportions_ztest(success_counts, total_counts)\n",
    "    return {\"stat\": stat, \"p_value\": p_value}\n",
    "\n",
    "def run_fisher_exact_test(df, variant_column, metric_column):\n",
    "    contingency_table = pd.crosstab(df[variant_column], df[metric_column])\n",
    "    stat, p_value = fisher_exact(contingency_table)\n",
    "    return {\"stat\": stat, \"p_value\": p_value}\n",
    "\n",
    "#\n",
    "\n",
    "def run_anova(df, variant_column, metric_column):\n",
    "    groups = [df[df[variant_column] == variant][metric_column] for variant in df[variant_column].unique()]\n",
    "    stat, p_value = f_oneway(*groups)\n",
    "    return {\"stat\": stat, \"p_value\": p_value}\n",
    "\n",
    "def run_welch_anova(df, variant_column, metric_column):\n",
    "    res = pg.welch_anova(dv=metric_column, between=variant_column, data=df)\n",
    "    return res\n",
    "\n",
    "def run_kruskal_wallis(df, variant_column, metric_column):\n",
    "    groups = [df[df[variant_column] == variant][metric_column] for variant in df[variant_column].unique()]\n",
    "    stat, p_value = kruskal(*groups)\n",
    "    return {\"stat\": stat, \"p_value\": p_value}\n",
    "\n",
    "def run_chi_square(df, variant_column, metric_column):\n",
    "    contingency_table = pd.crosstab(df[variant_column], df[metric_column])\n",
    "    stat, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "    return {\"stat\": stat, \"p_value\": p_value}\n",
    "\n",
    "\n",
    "\n",
    "def run_tukey_hsd(df, variant_column, metric_column):\n",
    "    tukey = pairwise_tukeyhsd(endog=df[metric_column], groups=df[variant_column], alpha=0.05)\n",
    "    return tukey.summary()\n",
    "\n",
    "def run_games_howell(df, variant_column, metric_column):\n",
    "    res = pg.pairwise_gameshowell(data=df, dv=metric_column, between=variant_column)\n",
    "    return res\n",
    "\n",
    "def run_dunn_test(df, variant_column, metric_column):\n",
    "    res = sp.posthoc_dunn(df, val_col=metric_column, group_col=variant_column, p_adjust='bonferroni')\n",
    "    return res\n",
    "\n",
    "def apply_bonferroni_correction(p_values_matrix):\n",
    "    p_values = p_values_matrix.values.flatten()\n",
    "    mask = ~np.isnan(p_values)\n",
    "    adjusted_p_values = multipletests(p_values[mask], method='bonferroni')[1]\n",
    "\n",
    "    corrected_matrix = p_values_matrix.copy()\n",
    "    corrected_matrix.values[mask] = adjusted_p_values\n",
    "    return corrected_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ab_tests(test_config, df, variant_column, metric_column):\n",
    "    results = {}\n",
    "\n",
    "    # Continuous\n",
    "    if test_config.get(\"use_t_test\", False):\n",
    "        results[\"t_test\"] = run_t_test(df, variant_column, metric_column)\n",
    "    if test_config.get(\"use_welchs_t_test\", False):\n",
    "        results[\"welchs_t_test\"] = run_welchs_t_test(df, variant_column, metric_column)\n",
    "    if test_config.get(\"use_mann_whitney_u_test\", False):\n",
    "        results[\"mann_whitney_u_test\"] = run_mann_whitney(df, variant_column, metric_column)\n",
    "\n",
    "    # Proportions\n",
    "    if test_config.get(\"use_fisher_exact_test\", False):\n",
    "        results[\"fisher_exact_test\"] = run_fisher_exact_test(df, variant_column, metric_column)\n",
    "    if test_config.get(\"use_two_proportion_z_test\", False):\n",
    "        results[\"two_proportion_z_test\"] = run_two_proportion_z_test(df, variant_column, metric_column)\n",
    "\n",
    "    # Continuous - Multiple Variants\n",
    "    if test_config.get(\"use_anova_test\", False):\n",
    "        results[\"anova_test\"] = run_anova(df, variant_column, metric_column)\n",
    "    if test_config.get(\"use_welch_anova_test\", False):\n",
    "        results[\"welch_anova_test\"] = run_welch_anova(df, variant_column, metric_column)\n",
    "    if test_config.get(\"use_kruskal_wallis_test\", False):\n",
    "        results[\"kruskal_wallis_test\"] = run_kruskal_wallis(df, variant_column, metric_column)\n",
    "\n",
    "    # Proportions - Multiple Variants\n",
    "    if test_config.get(\"use_pearson_chi_square_test\", False):\n",
    "        results[\"pearson_chi_square_test\"] = run_chi_square(df, variant_column, metric_column)\n",
    "\n",
    "    return results\n",
    "\n",
    "def run_post_hoc_tests(test_config, df, variant_column, metric_column):\n",
    "    results = {}\n",
    "\n",
    "    if test_config.get(\"use_tukey_hsd_test\", False):\n",
    "        results[\"tukey_hsd_test\"] = run_tukey_hsd(df, variant_column, metric_column)\n",
    "    if test_config.get(\"use_games_howell_test\", False):\n",
    "        results[\"games_howell_test\"] = run_games_howell(df, variant_column, metric_column)\n",
    "\n",
    "    if test_config.get(\"use_dunn_test\", False):\n",
    "        dunn_results = run_dunn_test(df, variant_column, metric_column)\n",
    "        results[\"dunn_test\"] = dunn_results\n",
    "        if test_config.get(\"use_bonferroni_correction\", False):\n",
    "            results[\"bonferroni_correction\"] = apply_bonferroni_correction(dunn_results)\n",
    "\n",
    "    if test_config.get(\"use_pearson_chi_square_test\", False):\n",
    "        z_test_results = run_two_proportion_z_test(df, variant_column, metric_column)\n",
    "        results[\"two_proportion_z_test\"] = z_test_results\n",
    "        if test_config.get(\"use_bonferroni_correction\", False):\n",
    "            results[\"bonferroni_correction\"] = apply_bonferroni_correction(z_test_results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ab_test_config['use_mann_whitney_u_test'] = False\n",
    "#ab_test_config['use_fisher_exact_test'] = True\n",
    "ab_test_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_p_value(json_data):\n",
    "    first_key = next(iter(json_data))\n",
    "    p_value = json_data[first_key].get(\"p_value\")\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "ab_test_results = run_ab_tests(ab_test_config, selected_df, variant_column, metric_column)\n",
    "print(ab_test_results)\n",
    "if num_variants > 2:\n",
    "    p_value = extract_p_value(ab_test_results)\n",
    "    print(p_value)\n",
    "    if p_value < alpha:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDITIONAL TECHNIQUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEGMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Segmentation\n",
    "use_welchs_t_test_segmentation = False # for comparing two segments (A-New vs. B-New), continuous, normal, can unbalanced\n",
    "use_mann_whitney_u_test_segmentation = False # for comparing two segments (A-New vs. B-New), continuous, not normal, can unbalanced\n",
    "use_two_proportion_z_test_segmentation = False # for comparing two segments (A-New vs. B-New), proportions, large sample\n",
    "use_fisher_exact_test_segmentation = False # for comparing two segments (A-New vs. B-New), proportions, small sample\n",
    "\n",
    "## Interaction Tests, if discrepancies between segments\n",
    "use_anova_interaction_test_segmentation = False # to test interaction effect between variant and segment, continuous, normal\n",
    "use_welch_anova_interaction_test_segmentation = False # to test interaction effect between variant and segment, continuous, normal, can unbalanced\n",
    "use_kruskal_wallis_interaction_test_segmentation = False # to test interaction effect between variant and segment, continuous, not normal\n",
    "use_logistic_regression_interaction_test_segmentation = False # to test interaction effect for proportions, equivalent to ANOVA for categorical data\n",
    "# if true, post hoc with tukey, games howell or dunn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ab-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
